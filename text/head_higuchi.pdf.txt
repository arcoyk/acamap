alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

Flying Head: A Head Motion
Synchronization Mechanism
for Unmanned Aerial Vehicle Control
Keita Higuchi
Interdisciplinary Information

Jun Rekimoto

Abstract

Interfaculty Initiative in

We propose an unmanned aerial vehicle (UAV) control
mechanism, called a “Flying Head” which synchronizes
a human head and the UAV motions. The accurate
manipulation of UAVs is difficult as their control
typically involves hand-operated devices. We can
incorporate the UAV control using human motions such
as walking, looking around and crouching. The system
synchronizes the operator and UAV positions in terms
of the horizontal and vertical positions and the yaw
orientation. The operator can use the UAV more
intuitively as such manipulations are more in accord
with kinesthetic. Finally, we discuss flying telepresence
applications.

Studies, The University of Tokyo Information Studies,
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku
7-3-1 Hongo, Bunkyo-ku Tokyo
Tokyo 113-0033 Japan
113-0033 Japan

Japan Society for the Promotion
of Science
6 Ichiban-cho, Chiyoda-ku
Tokyo 102-8471 Japan
khiguchi@acm.org

Sony Computer Science
Laboratories, Inc.
3-14-13 Higashigotanda,
Shinagawa-ku Tokyo
141-0022 Japan
rekimoto@acm.org

Author Keywords
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy

Unmanned Aerial Vehicle, Control Mechanism, Flying
Telepresence.

ACM Classification Keywords
H .5.1 Multimedia Information Systems

otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
CHI 2013 Extended Abstracts, April 27–May 2, 2013, Paris, France.
Copyright © 2013 ACM 978-1-4503-1952-2/13/04...$15.00

2029

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

Introduction

Figure 1. The Flying Head: The
system synchronizes human head
motions with those of an unmanned
aerial vehicle (UAV).

Remotely operated robots can be used in such
applications as telecommunications [1] and disaster
relief [2]. The technologies involved in remote
operation are collectively called telepresence, a field for
which there is a growing body of research and
development. An unmanned aerial vehicle (UAV) is a
flying robot that can move freely through the air and
can circumvent poor ground conditions such as uneven
roads and non-graded areas. Following the TohokuPacific Ocean Earthquake, human-controlled UAVs were
used to survey the damage at the Fukushima Dai-1
nuclear plant. In recent research, the UAV can capture
a 3D-reconstruction image in indoor and outdoor
environments using mounted cameras [3]. Openhardware UAVs have also enhanced projects such as
MikroKopter and Quaduino.
Currently, many UAV systems are controlled by handoperated devices such as proportional R/Cs, joysticks,
and keysets. Such devices are not instinctively easy to
manipulate, and long training times are needed for
precision flying. Precision control on the part of the
operators necessary to suitably control the flight
parameters such as the altitude, pitch, roll, and yaw in
real time. Proportional R/C, which s a typical UAV
control system, involves the use of several sticks and
switches for setting the flight parameters and
performing other tasks.
This paper addresses the challenge of revealing the
possibility of telepresence to a UAV. We propose the UAV
control mechanism, called the “Flying Head”. The Flying
Head synchronizes user head motions to the
movements of a flying robot, which can be easily

manipulated through such motions as walking, looking
around and crouching (Figure1). “Flying Telepresence”
is the term we use for the remote operation of a flying
surrogate robot in such a way that the operator's “self”
seemingly takes control. Flying telepresence can be
implemented under a variety of conditions: indoors or
outdoors, and in tight or open spaces.

Flying Head
The Flying Head is a UAV control mechanism that uses
human head motions to generate similar UAV
movements. Using this method, the operator wears a
head-mounted display (HMD) and moves their body.
Through such body motions, the operator can
intuitively manipulate the UAV as the movement of the
vehicle is mapped to the user's kinesthetic imagery. For
example, the operator can control the horizontal
movement. In addition, when the operator crouches, it
causes the UAV to lower itself to the ground. When the
operator rotates their head, the system rotates the
body of the UAV.
Superiority of body control
The characteristics of the Flying Head for introducing
human body motions for UAV control are as follows.

Operators can concurrently determine the UAV's
position and camera orientation.

Operators can recognize the movement distance of
the UAV based on the kinesthetic imagery.

A Flying Head operator can easily manipulate a UAV
using a set of head motions to control its location and
orientation. The operator should manipulate parallel

2030

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

altitudes or fly at altitudes lower than human stature,
making postural control of flight uncomfortable or even
impossible. We combine Flying Head with other control
method for altitude control. We focus a small device
that does not constrain human body movement. The
UAV can move high altitude as easy manipulation of the
device.

Figure 2. System mechanism: A quadcopter has four control
parameters such as pitch, roll, yaw and throttle.

Figure 3. System configuration: The
prototype system incorporates a
position measurement system using
eight motion capture cameras, a miniUAV, and an HMD.

parameters such as the horizontal and vertical
movements and orientation of the UAV. UAV operation
involves the simultaneous control of several parameters,
including the pitch, roll, yaw, and altitude. The method
used to input the representative movements into the
manipulated device are limited by the ability of the
system to accept input in parallel with the mapped
behaviors. With the Flying Head, we have adapted
human motions such as walking and looking around for
setting the flight parameters, allowing the operator to
input parallel control parameters of the UAV
simultaneously.
The operator understands the UAV movement distance
as the UAV is synchronized to the operator's body
motions. The Flying Head uses the operator's
kinesthetic information to control the UAV motion,
mitigating the need for vestibular system feedback.
Filling in the Gaps
However, a UAV does not fully synchronize to all human
motions, owing to human physical limitations with
respect to UAV flight capability. UAVs can soar to high

Latency indicates an unpreventable difference between
the position of the UAV and the operator's head.
Latency complicates an instinctive manipulation as the
operator cannot properly recognize the current position
of the UAV. However, latency cannot be fully smoothed
out cause of transmission speed and difference in
motion performance.
The Flying Head provides a latency representation to
the operator using an image processing method for a
better understanding of the UAV position. The system
applies a transformation of the feedback image when
latency occurs.

Prototype System
We developed a prototype system of the Flying Head,
which synchronizes an operator and a UAV motions that
include x, y and z position and yaw orientation (Figure
2). The prototype system incorporates a positioning
measurement system, a mini-UAV, and an HMD. For
control, the system requires accurate point information
for both the human operator and the UAV; therefore,
we adopted a point-recognition system that measures
the location information of the operator's head and the
UAV. Figure 3 shows the configuration of the system
control using this point information. As the figure
indicates, an operator wears an HMD for a

2031

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

representation of the UAV's camera image, allowing the
operator to control successive motions of the UAV.
To synchronize the operator's body motion with that of
the UAV, the system requires accurate point
information. We used OptiTrack as an optical motion
capture system for the positional measurements. An
OptiTrack S250e IR camera with a high frame-rate can
capture 120 frames per second, and motion capturing
allows the calculation of the marker's position to
accuracy of 1 mm. We captured the marker motions by
installing eight cameras in a room divided into human
and UAV areas, each of which was 3.0 m long by 1.5 m
wide.
We adopted an AR.Drone, which is a small quadcopter
with four blade propellers that can be controlled using
wireless communication, as a flying telepresence robot.
We set trackable markers with motion capture
capability on the AR.Drone to provide spatial (x-, y-,
and z-coordinate) and angle-of-rotation (pitch, roll, and
yaw) information. AR.Drone has a front camera and
underside cameras; The Flying Head uses the front
camera for visual feedback.
The AR. Drone has four control parameters; pitch, roll,
yaw and throttle (Figure 2). The pitch is the front and
back movement parameter, and the roll is right and left
movement parameter. When changing the yaw
parameter, the AR. Drone rotates on site, when
changing the throttle parameter, the AR. Drone moves
up or down. The system sends the control parameter to
the AR. Drone once every 30 milliseconds.
An operator wears a device with an HMD to represent
images captured from the UAV cameras. For the HMD,

we used a Sony HMZ-T1, which provides high-definition
(HD) image quality. The HMD is attachable to markers
that can track the operator's body motions as the
system can recognize body motions only through timeline point information provided by trackable markers.
The user locates the next manipulation of the UAV
based on visual feedback from the previous
manipulation. The wearable device is connected using
12 m long HDMI and power source cables that extend
to the ceiling to remain out of the way of the body
motions of the human operators. The inner camera of
the AR.Drone has a QVGA resolution of 320 x 240
pixels, with a capture speed of 30 frames per second.
This camera is oriented for the viewpoint from the front
side of the AR.Drone.
Control of horizontal movement
The system uses the position information of the
operator and UAV generated from the positioning
measurement system. The positioning parameters
include the plane point [x, y, z] and one direction [θ].
Horizontal movement control does not use the height
direction. Therefore, the system sets up the pitch (front
and back), roll (right and left) and yaw (rotation).
The system obtains the location points of the HMD (Hi)
and UAV (Ui) at time i (i = 0 ... k). The system
calculates the different Di in Hi at each time.

H i = {xi , yi ,θ i }

(i = 0..n)

(1)

U i = {xi , yi ,θ i }

(i = 0..n)

(2)

Di = H i − U i

(3)

2032

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

In time i, pitchi, rolli and yawi are calculated based on
the following equation.

⎛ pitch ⎞ ⎛ cosθU
⎜⎝ roll ⎟⎠ = ⎜⎜ − cosθ
U
⎝

yaw =

θD
π

sin θU ⎞ ⎛ yD ⎞
⎟
sin θU ⎟⎠ ⎜⎝ x D ⎟⎠

(4)

(5)

Additionally, the system estimates the future position
(expression 6) of the UAV based on its position history
for a fast-convergent UAV movement. The system has
led to a transformation of the control condition
(expression 7,8) in which the future position is greater
than the current position (C: constant).
Figure 4. Latency representation
method: The system provides a
latency representation to the operator
for understanding the UAV position
using an image processing.

Fi+1 = U i + (Ui − U i−1 )Δt

(6)

pitch = − pitch × C

(7)

roll = −roll × C

(8)

Altitude control
The Flying Head provides two methods for UAV altitude
control that are equal control and a combination of a
device. Equal control is used to move the UAV up and
down the same distance as the operator's head moves;
for example, if the operator lowers their head by 20 cm,
the UAV descends by 20 cm. This provides what many
operators consider a highly sensitive degree of control;
however, it means that the UAV cannot exceed the
vertical height of the operator.

For a combination of the device, the operator can use a
combination of body motions and the control device but
only for altitude control. Initially, the altitude baseline
is the head height of the operator, and the device can
switch its baseline height. We adopted a Wii remote
controller connected to a PC through Bluetooth. We
map the altitude of the UAV to the remote controller's
arrow keys.
Latency representation method
Figure 4 (no-latency) shows a no-latency visual
feedback image from the inner camera of the UAV. The
system zooms in on the image when the operator is in
front of the UAV's position, and the system zooms out
from the image under a reverse situation. During a
period of latency, the system translates the image in an
effort to represent its left, right, top, and bottom areas
more properly.

User Study
To review the operability of the Flying Head mechanism,
we performed a user study of its capturing ability. We
made a comparison between the Flying Head and a
device control method using two studies. For study 1,
the participants captured four static markers using the
inner camera of the UAV through both control methods.
For study 2, the participants captured a moving vehicle,
i.e. a Plarail toy train using each method.
For a comparison of the control mechanisms, we
adopted a joystick control with one stick and various
buttons. The participants manipulated the UAV’s
position using the joystick in the manner described in
the Control Horizontal Movement section. For the
joystick control, the participants wore an HMD for visual
feedback, which used a latency representation method.

2033

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

Figure 6. Result of Study 1: A comparison of the average time

extending to the ceiling and four 2D markers. We
configured the markers using numbers 1 through 4.
The participants captured the markers using the UAV
camera in numbered order. We placed the markers on
the pole in a counterclockwise fashion ranging in height
from 80 to 230 cm. When using the Flying Head, the
participants had to use equal control and combination
of the device for altitude control. Figure 5 (b) shows the
image from the inner camera of the UAV, with the
detection area of the markers framed in the red square.
Figure 5 (c) shows detection of the marker. The marker
had to be framed by a green square. We performed
three sessions of experiments for each participant. We
also used different marker positions for each session,
which the participants did not know in advance.

required for each participant during three sessions, where a
shorter time is better. The Flying Head was faster than the
joystick for every session. The average completion times for
Figure 5. Environment of study 1:

the three sessions were 40.8 s for the Flying Head and 80.1 s

The participants captured four visible

for the joystick. Error bars show the standard deviation.

markers using the Flying Head and
joystick control mechanism. We
measured and compared the
completion time of each method.

The joystick control differs from the Flying Head in that
only input method parameters such as the x, y, and z
positions and yaw orientation. Six people between the
ages of 23 and 25 with heights of 161 to 175cm
participated in this study. Three of the participants tried
using the Flying Head mechanism first, and the joystick
mechanism thereafter. The other participants tried the
joystick mechanism first, and the Flying Head
mechanism second.
User study 1
In this study, we measured the time of task completion.
The participants capture four visible markers using
each UAV control mechanism. Figure 5 (a) shows the
experimental environment, which includes a pole

Figure 6 shows a comparison of the average value of
every participant for all three sessions. The Flying Head
showed the highest time for all three sessions. The
average completion time for the three sessions was
40.8 s for the Flying Head and 80.1 s for the joystick
method. We conducted a paired t-test from the average
of each participant, which gave us a p-value of 0.007.
User Study 2
In study 2, we compared the accuracy of shooting a
moving vehicle when controlled by the Flying Head or a
joystick. The moving vehicle is a Plarail toy train. We
compared the time during which the UAV followed the
2D marker located on the back of the train. The Plarail
toy train drove on an elliptical course as shown Figure 7
(a). The marker was set upside down and toward the
direction of the Plarail toy train. The participants
tracked the marker by controlling the UAV while the
Plarail toy train ran five laps around the course. We

2034

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

the UAV while the Plarail was driven three laps before
the experiment started. The marker was 8.5 cm by 8.5
cm. We performed a visual check of the successful
shooting time for user study 2. Figure 7 (b) shows the
successful shooting, capturing four vertices of the
marker. Figure 7 (c) shows a failure in shooting, in
which the marker could not captured.

Figure 8. Result of Study 2: The result of study 2: This is a

Figure 8 shows a comparison of the average value for
all three sessions. The Flying Head showed the highest
rate of successful shooting for each session. The
average successful shooting rate was 59.3 % for the
Flying Head and 35.8 % for joystick. We again
conducted a paired t-test on the results of these
sessions, which gave us a p-value of 0.012.

comparison of the average value for all three sessions, which
Figure 7. Environment of study 2:
The participants captured a moving
vehicle using the two UAV control
mechanisms. We compared how long
the participants took to track the train
using both methods.

every session was 59.3%for the Flying Head and 35.8 % for
the joystick method. Error bars show the standard deviation.

measured how long the participants took to track the
train.Therefore, the percentage of successful shooting
time (Ps) can be formulated based on the entire
shooting time (Tall) and the amount of successful
shooting time (Ts) as shown in expression 9.

Ps =

Ts
Tall

(9)

The entire course was 221 cm long, and the velocity of
the Plarail train was 13.8 cm/s; each session lasted
about 80 s. In each session, the Plarail toy train was
driven five laps around the course. Once again, we
conducted three sets of experiments for each
participant. Before the experiment began, the
participants were allowed to set the starting position of

Questionnaire
We conducted a questionnaire survey the participants
regarding both the Flying Head and joystick method.
The questionnaire consisted of 8 questions, each of
which was evaluated on a scale of 1 to 5, with a higher
score indicating a better result. Table 1 shows question
items. As a result, the Flying Head received positive
answers to questions Q1 through Q7 (Figure 9).
Q1

Was control mechanism simple to control?

Q2

Could you control it properly?

Q3

Was Study 1 easy?

Q4

Was Study 2 easy?

Q5

Did it understand latency representation?

Q6

Did the latency interfere with the operation?

Q7

Did you enjoy the experiment?

Q8

Did you become tired with the experiment?

Table 1. Questionnaire item.

2035

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

Limitation
In an outdoor environment, the Flying Head cannot use
optical motion capturing for locating the UAV, owing to
sunlight or disturbances in the air. We intend to
develop a new localization system for outdoor use,
possibly involving the use of GPS, Wi-Fi, or ultrawideband technology. In light of its accuracy, we feel
that the use of an Ubisense ultra-wideband system as a
real-time locator may be a valid approach to this issue.

Figure 9. Result of the questionnaire: The questionnaire
consists of 8 items each of which was evaluated on a scale of 1
to 5, with a higher score is a better. Items with * are
significantly different between groups. Error bars show the
standard deviation.

Figure 10. Other control method for match-up Flying Head:
This mechanism is mapping human head inclination and the
UAV movement. Future work is needed to identify the
availability of this method with a combination of the Flying
Head.

Discussion
In this section, we discuss some plans for future
research and applications of flying telepresence

Combine other control method
In the study described herein, the UAV flights were
implemented only within ranges commensurate with
the distances walked by their operators. However, in
some telepresence exercises, the operator and the
robot will not move at equal scales, in which case the
system should be able to perform distance scaling. For
instance, if the operational range of the robot is threetimes that of the operator, 1 m walked by the operator
would be mapped to 3 m of UAV movement. We plan to
expand the Flying Head system to include such
scalability and to measure its usability, as well as to
combine and creatively use additional manipulation
methods.
Thus, we developed other a UAV control mechanism for
match-up Flying Head mechanism. This mechanism is
mapping human head inclination and the UAV
movement (Figure 10). This method is switched by Akey of Wii-Remote controller, which similarly uses
“Altitude control”. When the operator is an inclination of
the body in front from switching head position, the UAV
moves forward. When the operator raises head position,
the UAV also continually raises. Each yaw rotation
synchronizes the operator and the UAV as with the
Flying Head mechanism. This method does not have

2036

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

limitation of moving range because it does not need the
UAV position information. Future work is needed to
identify the availability of this method with a
combination of the Flying Head.
Other feedback
We have considered other feedback methods, such as
sound or haptics, to map the UAV modes to the
operator’s senses. One participant in the experiment
said that they “did not sense a clash of the UAV with
the object.” The operator determines their next body
motion based on sentient sensations provided by
kinesthetic information or visual feedback. Lam et al.
designed the UAV collision avoidance method using
haptic interface in a virtual environment [4]. We plan to
use a feedback system to provide sentient
environmental information from the UAV’s periphery by
means of a combination of depth cameras, nearby
sensors, and sensor types.
Capturing platform
The VR system can set the location and orientation as a
virtual camera using instinctive devices. Ware et al.
proposed the hand manipulation of a virtual camera [5].
We believe that the Flying Head can be used to
manipulate a physical camera system, such as digital
movie cameras used in motion pictures and game
creation for shooting stereoscopic 3D images. The
Flying Head has the possibility to be used in future
video content creation systems, in which a cameraman
would capture the action through the highly effective
employment of positioning and orientation
Teleoperation
Flying telepresence can also be used to facilitate
remote operations. For example, UAVs with

manipulation equipment can be employed in tasks such
as disaster relief or high-altitude construction. However,
current UAVs lack free manipulation equipment
comparable to the hands of a human operator. NASA
has developed Robonaut, a telepresence robot to be
used for exterior work in outer space [6]. Robonaut has
two arms that synchronize to the operator’s hand
motions. In their current research, Lindsay et al.
demonstrated the construction of a cubic structure
using mini-UAVs with a crane [7]. As Flying Head
operators would not be constrained to the use of their a
hand, they would be able to use a hand free body
motions to control the UAVs.
Sport training and healthcare
Flying telepresence may also provide an out-of-body
experience, or the sensation of leaving one’s own body.
When we demonstrated a Flying Head prototype to a
large audience (more than 100 people), several
participants noted the novelty of the experience of
seeing themselves from outside their bodies, reflecting
the ability of flying telepresence operators to observe
themselves through UAV cameras. The Flying Head
may be applicable to the use of out-of-body vision to
promote correct postures for standing, walking, and
running for use in sports training, health promotion,
and rehabilitation. Additionally, the system can be used
as a remote learning platform with professionals such
as sports coaches, mentors, and teachers.

Related Work
Recent research of UAVs focuses controlling methods.
Quigley et al. described how devices for establishing UAV
control parameters, including PDAs, joysticks, and voicerecognition systems, can be used [8]. Giordano et al.
developed a situational-aware UAV control system that

2037

alt.chi: Experiences

CHI 2013: Changing Perspectives, Paris, France

provides vestibular and visual-sensation feedback using a
CyberMotion simulator [9]. This system represents UAV
motion information within the operator's vestibular system.
However, these gestures are essentially just a
replacement for a device input, and it is difficult to use
them for inputting parallel control parameters of the UAV.
Vries et al. developed a UAV with a toting head-slave
camera [10]. However, these operation methods require
long training times as parallel parameters need to be set.

Conclusion
Flying telepresence is a term used for the remote
operation of a flying surrogate robot in such a way that
the operator’s self” seemingly takes control. In this
paper, we propose a control mechanism, termed the
Flying Head, which synchronizes the motion of a human
head with the motion of the UAV. The operator can
manipulate the UAV more intuitively as such
manipulations are more in accord with kinesthetic
imagery. The results of this user study indicate that the
Flying Head provides an easy operation preferable to
that of a joystick. Finally, we discuss additional flying
telepresence applications, such as capturing platforms,
teleoperation, sports training, and healthcare.

Acknowledgements
This research was partially supported by Grant-in-Aid
for JSPS Fellows Number 24-10424.

References
[1] Lee, S. Automatic gesture recognition for intelligent
human-robot interaction. In Automatic Face and
Gesture Recognition, 2006. FGR 2006. 7th International
Conference on, IEEE (2006), 645–650.
[2] Kamegawa, T., Yamasaki, T., Igarashi, H., and
Matsuno, F. Development of the snake-like rescue robot.
In Robotics and Automation, 2004. Proceedings.

ICRA’04. 2004 IEEE International Conference on, vol. 5,
5081–5086, (2004).
[3] Wendel, A., Maurer, M., Graber, G., Pock, T., and
Bischof, H. Dense reconstruction on-the-fly. In
Computer Vision and Pattern Recognition (CVPR) 2012,
1450 –1457, (2012).
[4] Lam, T. Mung, Max Mulder, and Paassen M. M. van.
Haptic Interface For UAV Collision Avoidance, he
International Journal of Aviation Psychology Volume 17,
Issue 2, (2007).
[5] Ware, C., and Osborne, S. Exploration and virtual
camera control in virtual three dimensional
environments. In ACM SIGGRAPH Computer Graphics,
vol. 24, ACM (1990), 175–183.
[6] Bluethmann, W., Ambrose, R., Diftler, M., Askew,
S., Huber, E., Goza, M., Rehnmark, F., Lovchik, C., and
Magruder, D. Robonaut: A robot designed to work with
humans in space. Autonomous Robots 14, 2 (2003),
179–197.
[7] Lindsey, Q., Mellinger, D., and Kumar, V.
Construction of cubic structures with quadrotor teams.
Proc. Robotics: Science & Systems VII (2011).
[8] Quigley, M., Goodrich, M., and Beard, R. Semiautonomous human-uav interfaces for fixed-wing miniuavs. In Intelligent Robots and Systems, 2004.(IROS
2004). Proceedings. 2004 IEEE/RSJ International
Conference on, vol. 3, 2457–2462, (2004).
、
、
[9] Giordano, P., Deusch, H., Lachele, J., and Bulthoff,
H. Visual-vestibular feedback for enhanced situational
awareness in teleoperation of uavs. In Proc. of the AHS
66th Annual Forum and Technology Display (2010).
[10] Vries, Sjoerd C.; Padmos, P. Steering a simulated
unmanned aerial vehicle using a head-slaved camera
and hmd: effects of hmd quality, visible vehicle
references, and extended stereo cueing. In Proc. SPIE,
80–91.

2038

