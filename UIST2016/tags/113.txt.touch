Hand Gesture and On-body Touch Recognition
by Active Acoustic Sensing throughout the Human Body
Tomohiro Yokota
Tomoko Hashida
Waseda University
3-4-1 Okubo, Shinjuku-ku, Tokyo, Japan
t-yokota@akane.waseda.jp, hashida@waseda.jp
ABSTRACT

In this paper, we present a novel acoustic sensing technique
that recognizes two convenient input actions: hand gestures
and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around
the periphery of the wrist. Our approach can recognize hand
gestures and on-body touch concurrently in real-time and is
expected to obtain rich input variations by combining them.
We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on
the forearm, and touches on the back of the hand.

Figure 1. Appearance of the prototype: a) hand gesture recognition, b) touch recognition (on the forearm), c) general view
of the wristband, d) and the ring.
PROTOTYPE

Author Keywords

ACM Classification Keywords

Our system recognizes hand gestures and on-body touch by
analyzing the ultrasound wave propagated through the body.
The prototype device consists of a wristband and a ring and
has drop-proof aluminum housing transducers (Figure 1).

INTRODUCTION

Active Acoustic Sensing throughout the Human Body,
Especially from Forearm to Hand

Hand gestures; on-body touch; combined input; acoustic
sensing; machine learning.
H.5.2. User interfaces: Input devices and strategies.
The human body is a significant interaction platform for controlling wearable devices and smart furniture, which provide
specific feedback on the basis of the tactile sense of the body
itself. In research of the body as an interface, there are
roughly two kinds of approaches: recognizing hand gestures
and recognizing on-body touch. However, most preceding
works took up either one or the other. Our goal of this research is recognizing both hand gestures and on-body touch
concurrently. To do this, we focused on an acoustic sensing
technique, observing the ultrasound wave propagated in the
body. By using a similar acoustic method, Okawa et al. detected a finger gesture on the basis of its joint angle [4], and
Mujibiya et al. enabled 1D continuous touch on the body [1].
Unlike Mujibiya et al.’s work, we recognize touch only on a
discrete pre-learned position of the body. Our work is inferior in this respect, but it has great potential for rich input
variation by combining hand gestures and on-body touch.

In our implementation, we mount a transmitter on the wrist
perpendicularly. Then, we drive the transmitter by a specific
ultrasound signal and apply the oscillation to the human body
actively. The oscillation propagates from the wrist, and we
receive the wave by mounting receivers on around the periphery of the wrist. By observing the frequency spectrum of
the wave propagated in the body, we can sense the conditions
of the body as a medium that has relation to its transfer characteristics, such as the posture of the hand and the position
of the receivers.
Hand Gesture Recognition

For recognizing hand gestures, we use a receiver mounted
near the styloid process of the radius, which is about 30mm
away from a transmitter on the wrist (Figure 1a and c). A
user wears a wristband with the pair of transducers on one
side of the wrist like he/she wears an ordinary wristwatch.
To prevent slipping the transducers’ position, we put a 1mm-thick silicon rubber sheet between a sensor case and the
wrist. We adopted a hand gesture set that has 8 hand postures
(Figure 2a). The frequency spectrum of the received wave
changes with moving of joints and muscles, which attend on
the hand gesture. (Figure 3a).

Permission to make digital or hard copies of part or all of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for third-party components of
this work must be honored. For all other uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
UIST'16 Adjunct, October 16-19, 2016, Tokyo, Japan
ACM 978-1-4503-4531-6/16/10.
http://dx.doi.org/10.1145/2984751.2985721

Touch Recognition on Forearm and Back of the Hand

For recognizing on-body touch, the user touches his/her own
body with mounting a receiver worn as a ring on the opposite
hand from the band-wearing wrist (Figure 1b and d). We
adopted two touch sets, each of which has 9 touch positions
(3 × 3 dot matrix with 2cm intervals), on the fore113

procedure resulted in three datasets: 3200 data points in the
hand gesture set, and 4000 data points in each of the two
touch sets.
On each dataset, we trained on nine rounds and then tested
on the other round in all combinations, and averaged these
results per user. As results, we achieved a mean accuracy of
97.0% (SD = 0.80%) for the hand gestures, 95.7% (SD =
2.43%) for the touch on the forearm, and 97.3% (SD =
2.94%) for the touch on the back of the hand.

Figure 2. Hand gesture set (a) and two touch sets: on the forearm and on the back of the hand (b).

Accuracy of Touch Input Combined With Hand Gesture

arm and on the back of the hand, respectively (Figure 2b).
The frequency spectrum of the received wave changes with
the position of the receiver, due to the difference in distance
and internal tissues of the pathway. Figure 3b shows the forearm case.

For recognizing the touch input combined with the specific
hand gesture (except ‘No gesture’), our system needs additional touch datasets in which the user keeps the left hand in
the specific gesture during the data collection. The system
classifies these touches by changing their SVM model by adjusting to the classification results of the gesture. Our preliminary study suggested that touches on the forearm tend to differ little in terms of accuracy regardless of the left-hand gesture. Thus, when the touch on the forearm is combined with
a hand gesture, the accuracy can be guessed to become close
to the product of individual accuracy of the hand gesture and
the touch with ‘No gesture’.

Implementation

POTENTIAL APPLICATION

Figure 3. Alternation of power spectrums by hand gesture (a)
and touch on the forearm (b).

Applying the above-described input technologies, we can realize various types of interaction methods. In this paper, we
propose a new touch input, which can change the output of
the touch by hand gesture. We show two contents of this input. 1) Keypad: the user can change three input modes of the
on-body keypad (two alphabet modes and one numeric
mode) by hand gestures: No gesture, Spread, and Thumb
open (Figure 4a, b, and c). 2) Controller having plural targets: The user can control various objects by on-body touch
by choosing the operational target by hand gestures (Figure
4d and e).

We used Murata MA40MF14-0B transducers and drove
them by using a USB audio interface, Roland OCTACAPTURE. For signal processing, we used the MATLAB.
The transmitter was driven by the swept frequency signal,
whose frequency increased linearly from 20kHz to 40kHz in
about 30ms at a 96kHz sampling rate, and the driving voltage
was about 10mVrms. Then, an ultrasound wave 69.7dBSPL
on average was applied to the body. This output was lower
than the safe levels of SPL in this sweep range [3]. Each received wave was amplified by Toshiba TA7252AP and analyzed by FFT, which was a common sampling rate and 5120point hann window. The total power consumption of the amplifier is about 0.54W. For classifications, we collected 100
features from the frequency spectrum at a 200Hz interval in
the sweep range. We used a SVM implementation provided
by the LIBSVM in the MATLAB (RBF kernel with default
parameters) [2]. All processing for hand gestures and touch
(FFT + SVM classifier) was able to run concurrently.
EVALUATION

We evaluated accuracy for each hand gesture and touch. We
recruited 4 participants (1 female) with a mean age of 23, all
right handed. They wore the wristband on their left forearm
and put the arm in front of their chest. First, we collected data
of the hand gestures and then the two touches on the forearm
and the back of the hand. For a single round of the data collection, we designated once all actions from 8 hand gestures
or 10 touches (‘No touch’ + 9 positions, draw the matrix on
the skin with a pen) in randomized order and collected 10
data points from each action. We made the participants put
their left forearm down during intervals between rounds and
collected 10 rounds in total. During the two touches, the participants kept their left hand posture as ‘No gesture.’ This

Figure 4. Keypad of a) lowercase mode, b) uppercase mode,
and c) numeric mode; and Controller targets d) notebook PC
(timer application) and e) display.
CONCLUSION

We recognized hand gestures and on-body touch concurrently in real-time by active acoustic sensing. We will evaluate the combined input as future work.
ACKNOWLEDGEMENTS

This paper is a part of the outcome of research performed
under a Waseda University Grant for Special Research Projects (Project number: 2015A-502).

114

REFERENCES

1.

Adiyan Mujibiya, Xiang Cao, Desney S. Tan, Dan
Morris, Shwetak N. Patel, and Jun Rekimoto. The
Sound of Touch: On-body Touch and Gesture Sensing
Based on Transdermal Ultrasound Propagation. In
Proc. ITS '13, 189-198, 2013.

2.

Chih-Chung Chang, and Chih-Jen Lin. LIBSVM: A Library for Support Vector Machines. Available online
at: https://www.csie.ntu.edu.tw/~cjlin/libsvm/. Accessed at July 4th, 2016.

115

3.

Non-Ionizing Radiation Section Bureau of Radiation
and Medical Devices Department of National Health
and Welfare. Guidelines for the Safe Use of Ultrasound: Part II - Industrial and Commercial Applications. Available online at: http://www.hc-sc.gc.ca/ewhsemt/pubs/radiation/safety-code_24-securite/indexeng.php. Accessed at July 4th, 2016.

4.

Yuya Okawa, and Kentaro Takemura. Haptic-enabled
Active Bone-Conducted Sound Sensing. In Proc.
UIST'15, 87-88, 2015.

