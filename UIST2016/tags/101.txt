Ballumiere: Real-Time Tracking and Projection for
High-Speed Moving Balls
Shio Miyafuji, Masato Sugasaki, Hideki Koike
Tokyo Institute of Technology
2-12-1, Ookayama, Meguro-ku, Tokyo 152-8550, JAPAN
{miyafuji.s.aa, sugasaki.m.ab}@m.titech.ac.jp, koike@cs.titech.ac.jp

Figure 1. Projection to balls thrown in the air and bouncing at a floor.

ABSTRACT

Projection onto moving objects has a serious slipping
problem due to delay between tracking and projection.
We propose a new method to overcome the delay problem, and we succeed in increasing the accuracy of projection. We present Ballumiere as a demo for projection
to volleyballs and juggling balls.
CCS Concepts

•Human-centered computing → Mixed / augmented reality; •Computing methodologies →
Tracking;
Author Keywords

Dynamic projection mapping; prediction; tracking
INTRODUCTION

Recently projection mapping has attracted much attention. For example, Shader Lamps [4] enables to render
computer graphics to real objects in three-dimensional
space using projectors. However, almost all projection
mapping techniques only allow projecting to static objects or projection targets that move slowly. To explore
high quality experience by projection mapping, technology for projection onto moving objects is required. Lumipen [3] tracks and projects to a small object in real
time using a high-speed vision sensor and a projector
combination using a high-speed optical gaze controller.
This system successfully projects to a quickly moving
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

UIST’16 Adjunct October 16-19, 2016, Tokyo, Japan
c 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4531-6/16/10.
DOI: http://dx.doi.org/10.1145/2984751.2985717

101

Figure 2. System overview.

object, but only allows for one object to be a projection
target because its optical axis of the vision sensors and
the projector must be the same. Knibbe et al. [1] proposed the prediction method of using Kalman filter and
a memory look up model with a Kinect camera. Lumospheres [2] can track balls and project images on them
in real time using motion capture cameras and standard
off the shelf projectors. This work overcomes the problem of the latency between tracking and projection by
predicting the position of a ball using Kalman filter and
kinetic model combination. However, they still have the
problem of low projection accuracy when the movement
speed of a ball changes quickly. To address this issue,
we present a new method, the 3 frame prediction model,
and switching prediction models according to motion of
balls, and we succeed in minimizing the projection slipping (Figure 1).
TECHNOLOGY

Figure 2 illustrates the overview of our system. The
system consists of six cameras, two (or more) projectors
and a PC for calculating prediction of balls’s coordinates
and also for rendering images. To capture the position
of the ball, we used six motion tracking cameras (OptiTrack S250e) with 120 fps to track balls wrapped with
retroreflective sheets. The three-dimensional position of
the ball is computed immediately by the motion tracking
system and is sent to the PC for calculation of appropriate image rendering considering the viewing angle from
each projector.
Prediction

Since there is latency in the cameras, PCs, and projectors, the position of the ball captured by the camera is
different from the position to be projected to. Some existing works overcome this issue with only Kalman filter,
however they still have a problem with the projection

Kalman ﬁlter
Three frame method

Figure 4. Projection accuracy examples.
Figure 3. Adaptive model switching.

Figure 5. Comparison between Kalman filter
and our proposed approach.

slipping when the ball’s speed changes suddenly, for example when it is caught. To address this problem, we
switch between Kalman filter and the 3 frame feedback
prediction model (Figure 3) and succeed in minimizing
the projection slipping.
We design the 3 frame feedback model to overcome the
projection slipping occurring in existing research. Exp.1
shows predicted position of a ball in a projector space
Pn at frame n.
Pn = pn + Qn Rn

(1)

Qn = α1 dn + α2 (dn − dn−1 )
(
|dn | (|dn | < 0.001)
Rn =
1
(|dn | >= 0.001)

(2)
(3)

pn is the current position of a ball captured by cameras, α is a correction coefficient for delay occurring at
cameras, projectors and program, and dn is the distance
between the positions at present and at 1 frame before.
There is the error component for acceleration because
this system calculates acceleration by using data from
motion capture cameras. Weighting terms of the equation with α works to minimize the effect of errors. Pn ,
predicted position, is corrected by Qn , a function of dn
for a correction term of velocity and dn − dn−1 for that
of acceleration, and Rn , a function for weighting by distance. Weighting by Rn (Exp.3) reduces projection slipping time at points where movement speed changes.
To improve prediction accuracy, we switch prediction
models when movement speed of a ball changes. We
estimate the condition of ball motion by y-axis acceleration and use Kalman filter under the influence of gravity
and 3 frame feedback model under other conditions.
Evaluation

In the evaluation experiment, a ball is dropped from
135cm height with zero y-axis (vertical) initial velocity.
Video is captured by a camera(120fps) beside a projector that projects an image onto a ball, and we calculate
the accuracy of projection with Exp.4 each frame.
Accuracy[%] =

image projected area[pixel]
× 100
projection target area[pixel]

(4)

Figure 4 shows how the projection efficiency measures
a projected image. The projection efficiency of the left
ball which nothing is projected on is 0%, that of the middle ball whose projection is slipped by its radius is 40%
and that of the right ball which the same size image is

102

projected on perfectly is 100%. We assess our proposed
method of the combination of Kalman filter and 3 frame
feedback model. We also evaluate no prediction model,
Kalman filter with kinetic model and 3 frame feedback
model for comparison. The average projection accuracies are 51.89[%] with no prediction model, 70.26[%] with
Kalman filter and kinetic model, 86.08[%] with 3 frame
feedback model and 89.79[%] with the combination of
Kalman filter and 3 frame feedback model. Figure 5
shows comparison of the accuracy of “Kalman filter prediction model” and our proposed method. Especially at
the first points of ball moving and just after bouncing
points, our new model achieves high ratio of accuracy.
It is because projections can soon back to the correct
position with 3 frame feedback model.
CONCLUSION

In this paper, we proposed the combination of Kalman
filter and 3 frame feedback model as a new prediction
model for tracking and projection onto flying balls in real
time. The results of our evaluation experiments support
our approach being better than the previous Kalman
filter model. Our results get 89.79%, a high projection
accuracy, on a moving and bouncing ball and we increase
it by 19.35% from our previous model.
REFERENCES

1. Knibbe, J., Benko, H., and Wilson, A. D. Juggling
the effects of latency: Software approaches to
minimizing latency in dynamic projector-camera
systems. In Adjunct Proc. of the 28th Annual ACM
Symp. on User Interface Software & Technology,
UIST’15, (2015), 93–94.
2. Koike, H., and Yamaguchi, H. LumoSpheres:
Real-time tracking of flying objects and image
projection for a volumetric display. In Proc. of the
6th Augmented Human Conf., AH ’15, (2015), 93–96.
3. Okumura, K., Oku, H., and Ishikawa, M. Lumipen:
Projection-based mixed reality for dynamic objects.
In Proc. of the 2012 IEEE Int’l Conf. on Multimedia
and Expo, ICME ’12, IEEE CS (2012), 699–704.
4. Raskar, R., Welch, G., Low, K.-L., and
Bandyopadhyay, D. Shader Lamps: Animating real
objects with image-based illumination. In Proc. of
the 12th Eurographics Workshop on Rendering
Techniques, Springer-Verlag (2001), 89–102.

